{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning: Cliff Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ROWS = 5\n",
    "COLS = 10\n",
    "S = (4, 0)\n",
    "G = (4, 9)\n",
    "\n",
    "\n",
    "class Cliff:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.end = False\n",
    "        self.pos = S\n",
    "        self.board = np.zeros([5, 10])\n",
    "        # add cliff marked as -1\n",
    "        self.board[4, 1:9] = -1\n",
    "\n",
    "    def nxtPosition(self, action):\n",
    "\n",
    "        if action == \"up\":\n",
    "            nxtPos = (self.pos[0] - 1, self.pos[1])\n",
    "        elif action == \"down\":\n",
    "            nxtPos = (self.pos[0] + 1, self.pos[1])\n",
    "        elif action == \"left\":\n",
    "            nxtPos = (self.pos[0], self.pos[1] - 1)\n",
    "        else:\n",
    "            nxtPos = (self.pos[0], self.pos[1] + 1)\n",
    "        # check legitimacy\n",
    "        if nxtPos[0] >= 0 and nxtPos[0] <= 4:\n",
    "            if nxtPos[1] >= 0 and nxtPos[1] <= 9:\n",
    "                self.pos = nxtPos\n",
    "\n",
    "        if self.pos == G:\n",
    "            self.end = True\n",
    "            print(\"Game ends reaching goal\")\n",
    "        if self.board[self.pos] == -1:\n",
    "            self.end = True\n",
    "            print(\"Game ends falling off cliff\")\n",
    "\n",
    "        return self.pos\n",
    "\n",
    "    def giveReward(self):\n",
    "        # give reward\n",
    "        if self.pos == G:\n",
    "            return 20\n",
    "        if self.board[self.pos] == 0:\n",
    "            return -1\n",
    "        return -100\n",
    "\n",
    "    def show(self):\n",
    "        for i in range(0, ROWS):\n",
    "            print('-------------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, COLS):\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                if (i, j) == self.pos:\n",
    "                    token = 'S'\n",
    "                if (i, j) == G:\n",
    "                    token = 'G'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------------------------------------------')\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, exp_rate=0.1, lr=0.1, sarsa=True):\n",
    "        self.cliff = Cliff()\n",
    "        self.actions = [\"up\", \"left\", \"right\", \"down\"]\n",
    "        self.states = []  # record position and action of each episode\n",
    "        self.pos = S\n",
    "        self.exp_rate = exp_rate\n",
    "        self.lr = lr\n",
    "        self.sarsa = sarsa\n",
    "        self.state_actions = {}\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                self.state_actions[(i, j)] = {}\n",
    "                for a in self.actions:\n",
    "                    self.state_actions[(i, j)][a] = 0\n",
    "\n",
    "    def chooseAction(self):\n",
    "        # epsilon-greedy\n",
    "        mx_nxt_reward = -999\n",
    "        action = \"\"\n",
    "\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                current_position = self.pos\n",
    "                nxt_reward = self.state_actions[current_position][a]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.cliff = Cliff()\n",
    "        self.pos = S\n",
    "\n",
    "    def play(self, rounds=10):\n",
    "\n",
    "        for _ in range(rounds):\n",
    "            while 1:\n",
    "                curr_state = self.pos\n",
    "                cur_reward = self.cliff.giveReward()\n",
    "                action = self.chooseAction()\n",
    "\n",
    "                # next position\n",
    "                self.cliff.pos = self.cliff.nxtPosition(action)\n",
    "                self.pos = self.cliff.pos\n",
    "\n",
    "                self.states.append([curr_state, action, cur_reward])\n",
    "                if self.cliff.end:\n",
    "                    break\n",
    "            # game end update estimates\n",
    "            reward = self.cliff.giveReward()\n",
    "            print(\"End game reward\", reward)\n",
    "\n",
    "            # reward of all actions in end state is same\n",
    "            for a in self.actions:\n",
    "                self.state_actions[self.pos][a] = reward\n",
    "\n",
    "            if self.sarsa:\n",
    "                for s in reversed(self.states):\n",
    "                    pos, action, r = s[0], s[1], s[2]\n",
    "                    current_value = self.state_actions[pos][action]\n",
    "                    reward = current_value + self.lr * (r + reward - current_value)\n",
    "                    self.state_actions[pos][action] = round(reward, 3)\n",
    "            else:\n",
    "                for s in reversed(self.states):\n",
    "                    pos, action, r = s[0], s[1], s[2]\n",
    "                    current_value = self.state_actions[pos][action]\n",
    "                    reward = current_value + self.lr * (r + reward - current_value)\n",
    "                    self.state_actions[pos][action] = round(reward, 3)\n",
    "                    # update using the max value of S'\n",
    "                    reward = np.max(list(self.state_actions[pos].values()))  # max\n",
    "            self.reset()\n",
    "\n",
    "\n",
    "\n",
    "def showRoute(states):\n",
    "    board = np.zeros([5, 10])\n",
    "    # add cliff marked as -1\n",
    "    board[4, 1:9] = -1\n",
    "    for i in range(0, ROWS):\n",
    "        print('-------------------------------------------------')\n",
    "        out = '| '\n",
    "        for j in range(0, COLS):\n",
    "            token = '0'\n",
    "            if board[i, j] == -1:\n",
    "                token = '*'\n",
    "            if (i, j) in states:\n",
    "                token = 'R'\n",
    "            if (i, j) == G:\n",
    "                token = 'G'\n",
    "            out += token + ' | '\n",
    "        print(out)\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"sarsa training ... \")\n",
    "    ag = Agent(exp_rate=0.99, sarsa=True)\n",
    "    ag.play(rounds=500)\n",
    "\n",
    "    # Sarsa\n",
    "    ag_op = Agent(exp_rate=0)\n",
    "    ag_op.state_actions = ag.state_actions\n",
    "\n",
    "    states = []\n",
    "    while 1:\n",
    "        curr_state = ag_op.pos\n",
    "        action = ag_op.chooseAction()\n",
    "        states.append(curr_state)\n",
    "        print(\"current position {} |action {}\".format(curr_state, action))\n",
    "\n",
    "        # next position\n",
    "        ag_op.cliff.pos = ag_op.cliff.nxtPosition(action)\n",
    "        ag_op.pos = ag_op.cliff.pos\n",
    "\n",
    "        if ag_op.cliff.end:\n",
    "            break\n",
    "\n",
    "    showRoute(states)\n",
    "\n",
    "    print(\"q-learning training ... \")\n",
    "    ag = Agent(exp_rate=0.99, sarsa=False)\n",
    "    ag.play(rounds=500)\n",
    "\n",
    "    # Q-learning\n",
    "    ag_op = Agent(exp_rate=0)\n",
    "    ag_op.state_actions = ag.state_actions\n",
    "\n",
    "    states = []\n",
    "    while 1:\n",
    "        curr_state = ag_op.pos\n",
    "        action = ag_op.chooseAction()\n",
    "        states.append(curr_state)\n",
    "        print(\"current position {} |action {}\".format(curr_state, action))\n",
    "\n",
    "        # next position\n",
    "        ag_op.cliff.pos = ag_op.cliff.nxtPosition(action)\n",
    "        ag_op.pos = ag_op.cliff.pos\n",
    "\n",
    "        if ag_op.cliff.end:\n",
    "            break\n",
    "\n",
    "    showRoute(states)\n",
    "    \n",
    "#code-base inspired by https://towardsdatascience.com/reinforcement-learning-cliff-walking-implementation-e40ce98418d4\n",
    "#I do not claim (intellectual) ownership of this code! -HJFB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
